{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "NLP Reddit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NekrozQliphort/SarcasmDetectionReddit/blob/main/NLP_Reddit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFOGWVPW8t70"
      },
      "source": [
        "## Reddit Sarcasm Detection"
      ],
      "id": "gFOGWVPW8t70"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5emMNA6x8t73"
      },
      "source": [
        "### Import Libraries"
      ],
      "id": "5emMNA6x8t73"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yw4BK4rh8t73",
        "outputId": "7716830b-0ec1-4a7e-bff9-0ec4361fbbf7"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "print(os.getcwd())"
      ],
      "id": "yw4BK4rh8t73",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TB8Zx19h8t74"
      },
      "source": [
        "### Import CSV"
      ],
      "id": "TB8Zx19h8t74"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHmlFKjL8t75"
      },
      "source": [
        "training_csv_1 = pd.read_csv(\"train-balanced-sarcasm.csv\")"
      ],
      "id": "qHmlFKjL8t75",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NwSLGkn8t75"
      },
      "source": [
        "training_csv_1[\"comment\"] = training_csv_1[\"comment\"].astype(str)"
      ],
      "id": "-NwSLGkn8t75",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Klx4uo958t75",
        "outputId": "0a6d4123-d25a-49e0-b48b-ddf9e188d7eb"
      },
      "source": [
        "training_csv_1.head()"
      ],
      "id": "Klx4uo958t75",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>comment</th>\n",
              "      <th>author</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>score</th>\n",
              "      <th>ups</th>\n",
              "      <th>downs</th>\n",
              "      <th>date</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>parent_comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NC and NH.</td>\n",
              "      <td>Trumpbart</td>\n",
              "      <td>politics</td>\n",
              "      <td>2</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>2016-10</td>\n",
              "      <td>2016-10-16 23:55:23</td>\n",
              "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>You do know west teams play against west teams...</td>\n",
              "      <td>Shbshb906</td>\n",
              "      <td>nba</td>\n",
              "      <td>-4</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>2016-11</td>\n",
              "      <td>2016-11-01 00:24:10</td>\n",
              "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>They were underdogs earlier today, but since G...</td>\n",
              "      <td>Creepeth</td>\n",
              "      <td>nfl</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-09</td>\n",
              "      <td>2016-09-22 21:45:37</td>\n",
              "      <td>They're favored to win.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
              "      <td>icebrotha</td>\n",
              "      <td>BlackPeopleTwitter</td>\n",
              "      <td>-8</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>2016-10</td>\n",
              "      <td>2016-10-18 21:03:47</td>\n",
              "      <td>deadass don't kill my buzz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>I could use one of those tools.</td>\n",
              "      <td>cush2push</td>\n",
              "      <td>MaddenUltimateTeam</td>\n",
              "      <td>6</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>2016-12</td>\n",
              "      <td>2016-12-30 17:00:13</td>\n",
              "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  ...                                     parent_comment\n",
              "0      0  ...  Yeah, I get that argument. At this point, I'd ...\n",
              "1      0  ...  The blazers and Mavericks (The wests 5 and 6 s...\n",
              "2      0  ...                            They're favored to win.\n",
              "3      0  ...                         deadass don't kill my buzz\n",
              "4      0  ...  Yep can confirm I saw the tool they use for th...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ecVPRo48t76"
      },
      "source": [
        "### Exploratory Data Analysis"
      ],
      "id": "4ecVPRo48t76"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVtT2acI8t77",
        "outputId": "7190b74c-b114-452d-9350-b48d1d9a4e7e"
      },
      "source": [
        "print(f\"The total training data has {training_csv_1.author.nunique()} rows.\")\n",
        "training_csv_1.groupby(\"author\").mean()[\"label\"].value_counts()"
      ],
      "id": "IVtT2acI8t77",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total training data has 32690 rows.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.000000    17856\n",
              "1.000000    10666\n",
              "0.500000     3033\n",
              "0.333333      432\n",
              "0.666667      352\n",
              "0.250000       75\n",
              "0.750000       68\n",
              "0.400000       49\n",
              "0.600000       49\n",
              "0.200000       16\n",
              "0.800000       15\n",
              "0.428571       11\n",
              "0.833333        9\n",
              "0.571429        8\n",
              "0.375000        8\n",
              "0.625000        5\n",
              "0.714286        4\n",
              "0.166667        4\n",
              "0.285714        4\n",
              "0.555556        4\n",
              "0.545455        3\n",
              "0.444444        2\n",
              "0.384615        2\n",
              "0.720000        1\n",
              "0.363636        1\n",
              "0.657143        1\n",
              "0.533333        1\n",
              "0.458333        1\n",
              "0.380952        1\n",
              "0.526316        1\n",
              "0.542857        1\n",
              "0.142857        1\n",
              "0.777778        1\n",
              "0.437500        1\n",
              "0.466667        1\n",
              "0.818182        1\n",
              "0.461538        1\n",
              "0.642857        1\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY_wh6HC8t77"
      },
      "source": [
        "##### The authors is mostly 0.5 probability of each label, might consider dropping it"
      ],
      "id": "bY_wh6HC8t77"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGiIwfkj8t78",
        "outputId": "0e92008e-a222-44f6-dad3-4aa8bad425d7"
      },
      "source": [
        "print(f\"The total training data has {training_csv_1.subreddit.nunique()} rows.\")\n",
        "training_csv_1.groupby(\"subreddit\").mean()[\"label\"].value_counts()"
      ],
      "id": "yGiIwfkj8t78",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The total training data has 3682 rows.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.000000    1666\n",
              "1.000000     532\n",
              "0.500000     362\n",
              "0.333333     204\n",
              "0.250000      78\n",
              "            ... \n",
              "0.451613       1\n",
              "0.378238       1\n",
              "0.314815       1\n",
              "0.488889       1\n",
              "0.483146       1\n",
              "Name: label, Length: 285, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9UVPQPP8t78"
      },
      "source": [
        "##### Subreddit seems to provide more info than expected, should probably keep"
      ],
      "id": "Q9UVPQPP8t78"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "GJmj8XlD8t78",
        "outputId": "26826969-79e9-4c68-8767-030df243070d"
      },
      "source": [
        "training_csv_1[[\"ups\", \"downs\"]]"
      ],
      "id": "GJmj8XlD8t78",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ups</th>\n",
              "      <th>downs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41477</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41478</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41479</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41480</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41481</th>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>41482 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       ups  downs\n",
              "0       -1     -1\n",
              "1       -1     -1\n",
              "2        3      0\n",
              "3       -1     -1\n",
              "4       -1     -1\n",
              "...    ...    ...\n",
              "41477   -1     -1\n",
              "41478   -1     -1\n",
              "41479   -1     -1\n",
              "41480   -1     -1\n",
              "41481   -1     -1\n",
              "\n",
              "[41482 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldw1GSI_8t78"
      },
      "source": [
        "##### Notice how ups and downs seem to have a correlation? Lets test this theory out"
      ],
      "id": "ldw1GSI_8t78"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "QWrAvXFL8t79",
        "outputId": "0e3da348-fff3-4290-bc4b-c9ff71f37414"
      },
      "source": [
        "training_csv_1[training_csv_1[\"ups\"].apply(lambda x: -1 if x <= -1 else 0) != training_csv_1[\"downs\"]]"
      ],
      "id": "QWrAvXFL8t79",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>comment</th>\n",
              "      <th>author</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>score</th>\n",
              "      <th>ups</th>\n",
              "      <th>downs</th>\n",
              "      <th>date</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>parent_comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>0</td>\n",
              "      <td>My comment very similar to this went down a fu...</td>\n",
              "      <td>Schumarker</td>\n",
              "      <td>Android</td>\n",
              "      <td>-6</td>\n",
              "      <td>-6</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-09</td>\n",
              "      <td>2016-09-24 21:50:56</td>\n",
              "      <td>Badumm-tzz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>0</td>\n",
              "      <td>it really does</td>\n",
              "      <td>Horus_Krishna_2</td>\n",
              "      <td>radiohead</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-09</td>\n",
              "      <td>2016-09-14 20:07:04</td>\n",
              "      <td>As far as I know, someone's reddit history doe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414</th>\n",
              "      <td>0</td>\n",
              "      <td>Meh, my upper body blows his away.</td>\n",
              "      <td>GiveMeSomeIhedigbo</td>\n",
              "      <td>bodybuilding</td>\n",
              "      <td>-6</td>\n",
              "      <td>-6</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-09</td>\n",
              "      <td>2016-09-19 06:27:32</td>\n",
              "      <td>Do you Agree that this version is The BEST Ver...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>431</th>\n",
              "      <td>0</td>\n",
              "      <td>Such a shitty meme.</td>\n",
              "      <td>Geralt-of_Rivia</td>\n",
              "      <td>AdviceAnimals</td>\n",
              "      <td>-4</td>\n",
              "      <td>-4</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-09</td>\n",
              "      <td>2016-09-02 02:39:44</td>\n",
              "      <td>Front page post with 2000 comments and is 10 h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>454</th>\n",
              "      <td>0</td>\n",
              "      <td>This sub is for open ended questions, not yes ...</td>\n",
              "      <td>hunterz5</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>-3</td>\n",
              "      <td>-3</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-09</td>\n",
              "      <td>2016-09-10 01:54:47</td>\n",
              "      <td>Do you think IB/AP classes are truly worth it?...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38690</th>\n",
              "      <td>0</td>\n",
              "      <td>Wtf is this boob ribbon thing ?</td>\n",
              "      <td>powsm</td>\n",
              "      <td>anime</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-09</td>\n",
              "      <td>2016-09-27 17:28:36</td>\n",
              "      <td>I know that but those girls are mostly either ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39290</th>\n",
              "      <td>0</td>\n",
              "      <td>Idc, dumbasses prolly deserved it</td>\n",
              "      <td>PM_ME_UR_THIGH_HIGHS</td>\n",
              "      <td>BlackPeopleTwitter</td>\n",
              "      <td>-19</td>\n",
              "      <td>-19</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-09</td>\n",
              "      <td>2016-09-24 21:59:50</td>\n",
              "      <td>And you find that acceptable?!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39476</th>\n",
              "      <td>0</td>\n",
              "      <td>Well we'll talk about that next year lol, I'm ...</td>\n",
              "      <td>OrbisAlius</td>\n",
              "      <td>formula1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-09</td>\n",
              "      <td>2016-09-27 06:53:06</td>\n",
              "      <td>If next year or 2018 turns out to go well for ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40042</th>\n",
              "      <td>0</td>\n",
              "      <td>And yet, I feel less sympathetic.</td>\n",
              "      <td>Blazebow</td>\n",
              "      <td>todayilearned</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-09</td>\n",
              "      <td>2016-09-05 09:52:30</td>\n",
              "      <td>That doesn't justify what happened.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41275</th>\n",
              "      <td>1</td>\n",
              "      <td>As a SEA colony of China, why doesn't SG follo...</td>\n",
              "      <td>heronumberwon</td>\n",
              "      <td>singapore</td>\n",
              "      <td>-3</td>\n",
              "      <td>-3</td>\n",
              "      <td>0</td>\n",
              "      <td>2016-09</td>\n",
              "      <td>2016-09-27 23:17:57</td>\n",
              "      <td>Singapore accuses Chinese paper of fabricating...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>476 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       label  ...                                     parent_comment\n",
              "140        0  ...                                         Badumm-tzz\n",
              "204        0  ...  As far as I know, someone's reddit history doe...\n",
              "414        0  ...  Do you Agree that this version is The BEST Ver...\n",
              "431        0  ...  Front page post with 2000 comments and is 10 h...\n",
              "454        0  ...  Do you think IB/AP classes are truly worth it?...\n",
              "...      ...  ...                                                ...\n",
              "38690      0  ...  I know that but those girls are mostly either ...\n",
              "39290      0  ...                     And you find that acceptable?!\n",
              "39476      0  ...  If next year or 2018 turns out to go well for ...\n",
              "40042      0  ...                That doesn't justify what happened.\n",
              "41275      1  ...  Singapore accuses Chinese paper of fabricating...\n",
              "\n",
              "[476 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol3EJ3B48t79"
      },
      "source": [
        "##### Only 6.1% does not follow the rules, is downs worth keeping? Debatable I guess"
      ],
      "id": "Ol3EJ3B48t79"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFLTKtAz8t79"
      },
      "source": [
        "### Build model using Comment Column only (Unigram Model)"
      ],
      "id": "vFLTKtAz8t79"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCSwI0Uv8t79"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import SGDClassifier"
      ],
      "id": "UCSwI0Uv8t79",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I8o24rj8t7-"
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "id": "1I8o24rj8t7-",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_TNlNI28t7-"
      },
      "source": [
        "## Better abstraction\n",
        "\n",
        "class sklearnClassifier:\n",
        "    def __init__(self, model, data, label, fitBool = True):\n",
        "        self.model = model\n",
        "        if fitBool: self.fit(data, label)\n",
        "            \n",
        "    def fit(self, data, label):\n",
        "        self.model.fit(data, label)\n",
        "    \n",
        "    def score(self, X, y_true):\n",
        "        y_pred = self.model.predict(X)\n",
        "        print(f\"Accuracy score: {accuracy_score(y_true, y_pred)}\")\n",
        "        print(f\"Recall score: {recall_score(y_true, y_pred)}\")\n",
        "        print(f\"Precision score: {precision_score(y_true, y_pred)}\")\n",
        "        print(f\"F1 score: {f1_score(y_true, y_pred)}\")"
      ],
      "id": "2_TNlNI28t7-",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa27CNma8t7-"
      },
      "source": [
        "training_csv_1[\"comment\"] = training_csv_1[\"comment\"].apply(lambda x: x.lower())"
      ],
      "id": "wa27CNma8t7-",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg7pxRWt8t7_"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    training_csv_1[\"comment\"], \n",
        "    training_csv_1[\"label\"], \n",
        "    test_size = 0.2\n",
        ")"
      ],
      "id": "Gg7pxRWt8t7_",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0ZfcSRJ8t7_"
      },
      "source": [
        "def create_ngram_vectorizer(text_train, ngram_range = (1,1), **kwargs):\n",
        "    vectorizer = CountVectorizer(ngram_range = ngram_range, **kwargs)\n",
        "    vectorizer.fit(text_train)\n",
        "    return vectorizer"
      ],
      "id": "Q0ZfcSRJ8t7_",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmSdV2CS8t7_"
      },
      "source": [
        "unigram_vectorizer = create_ngram_vectorizer(X_train)"
      ],
      "id": "MmSdV2CS8t7_",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKrurWpK8t7_"
      },
      "source": [
        "X_train_transformed = unigram_vectorizer.transform(X_train)\n",
        "X_val_transformed = unigram_vectorizer.transform(X_val)"
      ],
      "id": "hKrurWpK8t7_",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPNvKJV28t8A"
      },
      "source": [
        "base_classifier = sklearnClassifier(SGDClassifier(), X_train_transformed, y_train)"
      ],
      "id": "nPNvKJV28t8A",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PNj_9Fy8t8A",
        "outputId": "24862631-cb51-4e13-8b1c-f1410ceb0771"
      },
      "source": [
        "print(\"Training: \")\n",
        "base_classifier.score(X_train_transformed, y_train)\n",
        "print(\"Validation: \")\n",
        "base_classifier.score(X_val_transformed, y_val)"
      ],
      "id": "_PNj_9Fy8t8A",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: \n",
            "Accuracy score: 0.8060569534428206\n",
            "Recall score: 0.6184512782512042\n",
            "Precision score: 0.8663967611336032\n",
            "F1 score: 0.7217225873400207\n",
            "Validation: \n",
            "Accuracy score: 0.681089550439918\n",
            "Recall score: 0.44682115270350564\n",
            "Precision score: 0.6573426573426573\n",
            "F1 score: 0.5320127343473647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KgpV_cy8t8A"
      },
      "source": [
        "### Now what? Bigrams and Trigrams, LETZ GO!!!"
      ],
      "id": "1KgpV_cy8t8A"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uODUkdjJ8t8A"
      },
      "source": [
        "# for i in range(1, 3): # Trigram is a bit slow so we'll bring that back later\n",
        "#     igram_vectorizer = create_ngram_vectorizer(X_train, ngram_range = (1,i))\n",
        "#     X_train_transformed = igram_vectorizer.transform(X_train)\n",
        "#     X_val_transformed = igram_vectorizer.transform(X_val)\n",
        "    \n",
        "#     base_classifier = sklearnClassifier(SGDClassifier(), X_train_transformed, y_train)\n",
        "    \n",
        "#     print(\"Training: \")\n",
        "#     base_classifier.score(X_train_transformed, y_train)\n",
        "#     print(\"Validation: \")\n",
        "#     base_classifier.score(X_val_transformed, y_val)\n",
        "#     print()"
      ],
      "id": "uODUkdjJ8t8A",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stcQBhLi8t8A"
      },
      "source": [
        "### Using TFIDF instead of just counting"
      ],
      "id": "stcQBhLi8t8A"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_q7YN_c8t8A"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "id": "H_q7YN_c8t8A",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btArSyfp8t8B"
      },
      "source": [
        "def create_tfidf_ngram_vectorizer(text_train, ngram_range = (1,1), **kwargs):\n",
        "    vectorizer = TfidfVectorizer(ngram_range = ngram_range, **kwargs)\n",
        "    vectorizer.fit(text_train)\n",
        "    return vectorizer"
      ],
      "id": "btArSyfp8t8B",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "o8UAlua48t8B"
      },
      "source": [
        "# for i in range(1,3):\n",
        "#     tfidf_igram_vectorizer = create_tfidf_ngram_vectorizer(X_train, ngram_range = (1,i))\n",
        "#     X_train_transformed = tfidf_igram_vectorizer.transform(X_train)\n",
        "#     X_val_transformed = tfidf_igram_vectorizer.transform(X_val)\n",
        "    \n",
        "#     base_classifier = sklearnClassifier(SGDClassifier(), X_train_transformed, y_train)\n",
        "    \n",
        "#     print(\"Training: \")\n",
        "#     base_classifier.score(X_train_transformed, y_train)\n",
        "#     print(\"Validation: \")\n",
        "#     base_classifier.score(X_val_transformed, y_val)\n",
        "#     print()"
      ],
      "id": "o8UAlua48t8B",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk3zKOaI8t8B"
      },
      "source": [
        "### Vector Representation Test"
      ],
      "id": "Xk3zKOaI8t8B"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aa8mS728t8B"
      },
      "source": [
        "### Abstraction for easier work\n",
        "class EmbeddingTechniques:\n",
        "    def __init__(self, method):\n",
        "        self.transformMethod = method\n",
        "    \n",
        "    def transform(self, X):\n",
        "        return self.transformMethod(X)"
      ],
      "id": "6aa8mS728t8B",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l5xOdXo8t8B"
      },
      "source": [
        "class EmbeddingTester:\n",
        "    def __init__(self, sklearnmodel):\n",
        "        self.list_of_techniques = {}\n",
        "        self.tokenized = {}\n",
        "        self.model = sklearnmodel\n",
        "        \n",
        "    def addEmbeddingTechniques(self, key, method, tokenized = False):\n",
        "        self.list_of_techniques[key] = method\n",
        "        self.tokenized[key] = tokenized\n",
        "        \n",
        "        \n",
        "    def testModel(self, X_train_transformed, y_train_true, X_test_transformed, y_test_true, text = None):\n",
        "        if text is not None: print(text)\n",
        "        self.model.fit(X_train_transformed, y_train_true)\n",
        "        print(\"Training: \")\n",
        "        self.model.score(X_train_transformed, y_train_true)\n",
        "        print()\n",
        "        print(\"Validation: \")\n",
        "        self.model.score(X_test_transformed, y_test_true)\n",
        "        print(\"-\" * 80)\n",
        "        \n",
        "    def test(self, X_train_untransformed, y_train_true, X_test_untransformed, y_test_true,\n",
        "            X_train_tokenized, X_test_tokenized):\n",
        "        for key, val in self.list_of_techniques.items():\n",
        "            if self.tokenized[key]:\n",
        "                X_train_transformed = val.transform(X_train_tokenized)\n",
        "                X_test_transformed = val.transform(X_test_tokenized)\n",
        "            else:\n",
        "                X_train_transformed = val.transform(X_train_untransformed)\n",
        "                X_test_transformed = val.transform(X_test_untransformed)\n",
        "            self.testModel(X_train_transformed, y_train_true, X_test_transformed, y_test_true, text = key)"
      ],
      "id": "8l5xOdXo8t8B",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EL1I3W38t8B"
      },
      "source": [
        "tester = EmbeddingTester(base_classifier)\n",
        "tester.addEmbeddingTechniques(\n",
        "    \"Count Vectorizer(No stopwords removal)\", \n",
        "    create_ngram_vectorizer(X_train, ngram_range = (1,2))\n",
        ")\n",
        "\n",
        "tester.addEmbeddingTechniques(\n",
        "    \"TFIDF Vectorizer(No stopwords removal)\", \n",
        "    create_tfidf_ngram_vectorizer(X_train, ngram_range = (1,2))\n",
        ")\n",
        "\n",
        "tester.addEmbeddingTechniques(\n",
        "    \"Count Vectorizer(With stopwords removal)\", \n",
        "    create_ngram_vectorizer(X_train, ngram_range = (1,2), stop_words='english')\n",
        ")\n",
        "\n",
        "tester.addEmbeddingTechniques(\n",
        "    \"TFIDF Vectorizer(With stopwords removal)\", \n",
        "    create_tfidf_ngram_vectorizer(X_train, ngram_range = (1,2), stop_words='english')\n",
        ")"
      ],
      "id": "-EL1I3W38t8B",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv6jRlo08t8C"
      },
      "source": [
        "## Thanks Rama, like srsly\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "id": "Bv6jRlo08t8C",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "kxLZaVtC8t8C",
        "outputId": "f6c9b1a1-6a77-4f56-e21a-2d97e9ca35b6"
      },
      "source": [
        "vector_size = 128\n",
        "word_tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "X_train_tokenized = [word_tokenizer.tokenize(text) for text in X_train]\n",
        "X_val_tokenized = [word_tokenizer.tokenize(text) for text in X_val]\n",
        "\n",
        "model = Word2Vec(X_train_tokenized, min_count = 1, vector_size= vector_size, workers = 3, window = 3, sg = 1)"
      ],
      "id": "kxLZaVtC8t8C",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-ba6a63be8cd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_val_tokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'vector_size'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7WQdAj98t8C"
      },
      "source": [
        "def transform(X_tokenized):\n",
        "    temp = np.matrix(\n",
        "        [np.mean([model.wv[i] if i in model.wv else np.array([0.0] * vector_size, dtype=np.float64) for i in tokens], axis = 0) for tokens in X_tokenized],\n",
        "        dtype=np.float64\n",
        "    )\n",
        "    return temp"
      ],
      "id": "l7WQdAj98t8C",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ENHvPFNb8t8C",
        "outputId": "bf7558a9-b935-4cef-d227-b81fd8edf76c"
      },
      "source": [
        "tester.addEmbeddingTechniques(\n",
        "    \"word2Vec Mean Embedding\", \n",
        "    EmbeddingTechniques(transform),\n",
        "    True\n",
        ")\n",
        "\n",
        "tester.test(X_train, y_train, X_val, y_val, X_train_tokenized, X_val_tokenized)"
      ],
      "id": "ENHvPFNb8t8C",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count Vectorizer(No stopwords removal)\n",
            "Training: \n",
            "Accuracy score: 0.9656772638240169\n",
            "Recall score: 0.922341608002964\n",
            "Precision score: 0.9927420641250598\n",
            "F1 score: 0.9562478392809126\n",
            "\n",
            "Validation: \n",
            "Accuracy score: 0.6835000602627456\n",
            "Recall score: 0.49376114081996436\n",
            "Precision score: 0.6431888544891641\n",
            "F1 score: 0.5586554621848739\n",
            "--------------------------------------------------------------------------------\n",
            "TFIDF Vectorizer(No stopwords removal)\n",
            "Training: \n",
            "Accuracy score: 0.7322886846466777\n",
            "Recall score: 0.3780659503519822\n",
            "Precision score: 0.9122116931879135\n",
            "F1 score: 0.5345766974015088\n",
            "\n",
            "Validation: \n",
            "Accuracy score: 0.682656381824756\n",
            "Recall score: 0.33481877599524656\n",
            "Precision score: 0.7409598948060486\n",
            "F1 score: 0.46122365459382036\n",
            "--------------------------------------------------------------------------------\n",
            "Count Vectorizer(With stopwords removal)\n",
            "Training: \n",
            "Accuracy score: 0.9313545276480337\n",
            "Recall score: 0.8415709522045202\n",
            "Precision score: 0.9878229103244325\n",
            "F1 score: 0.9088508322663252\n",
            "\n",
            "Validation: \n",
            "Accuracy score: 0.6621670483307219\n",
            "Recall score: 0.41325014854426617\n",
            "Precision score: 0.6268589454709329\n",
            "F1 score: 0.4981199641897941\n",
            "--------------------------------------------------------------------------------\n",
            "TFIDF Vectorizer(With stopwords removal)\n",
            "Training: \n",
            "Accuracy score: 0.6894681331927075\n",
            "Recall score: 0.26787699147832533\n",
            "Precision score: 0.8948019801980198\n",
            "F1 score: 0.41231822070145424\n",
            "\n",
            "Validation: \n",
            "Accuracy score: 0.6587923345787634\n",
            "Recall score: 0.2641117052881759\n",
            "Precision score: 0.7152051488334674\n",
            "F1 score: 0.38576697765241924\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-b0b15a767a52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtester\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_tokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_tokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-23b8c9cc710f>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, X_train_untransformed, y_train_true, X_test_untransformed, y_test_true, X_train_tokenized, X_test_tokenized)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_of_techniques\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0mX_train_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0mX_test_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_tokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-60940b55dc3a>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformMethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-68158aa9da1b>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(X_tokenized)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     temp = np.matrix(\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_tokenized\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     )\n",
            "\u001b[0;32m<ipython-input-28-68158aa9da1b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     temp = np.matrix(\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_tokenized\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     )\n",
            "\u001b[0;32m<ipython-input-28-68158aa9da1b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     temp = np.matrix(\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_tokenized\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pelEF_Oz8t8C"
      },
      "source": [
        "## Feature Engineering"
      ],
      "id": "pelEF_Oz8t8C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM-la-Rd8t8C"
      },
      "source": [
        "### Imports"
      ],
      "id": "jM-la-Rd8t8C"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBUBUIa38t8D",
        "outputId": "808c5c05-b66b-4052-ec7a-7b6ed61f8c10"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer, WordPunctTokenizer\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "porter_stemmer = PorterStemmer()\n",
        "word_tokenizer = TreebankWordTokenizer()\n",
        "word_tokenizer2 = WordPunctTokenizer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "id": "RBUBUIa38t8D",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_83QkO_8t8D"
      },
      "source": [
        "## tokenize cols\n",
        "## Define function to remove stopwords and tokenize comments and or parent_comments\n",
        "def create_stopwords_dict():\n",
        "  stopwords_dict = {}\n",
        "  for word in set(stopwords.words('english')):\n",
        "    stopwords_dict[word] = True\n",
        "  return stopwords_dict\n",
        "\n",
        "stop_words_dict = create_stopwords_dict()\n",
        "\n",
        "\n",
        "def remove_stopwords_and_tokenize(text):\n",
        "  try:\n",
        "    arr = word_tokenizer.tokenize(text)\n",
        "    arr = [word for word in arr if word not in stop_words_dict]\n",
        "    return arr\n",
        "  except:\n",
        "    print(text)\n",
        "    return []\n",
        "\n",
        "def remove_stopwords_and_tokenize_cols_in_dataset(dataset, cols):\n",
        "    for col in cols:\n",
        "        dataset[col] = dataset[col].apply(lambda x: remove_stopwords_and_tokenize(x))\n",
        "    return dataset"
      ],
      "id": "H_83QkO_8t8D",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBdkO8E38t8D"
      },
      "source": [
        "## After tokenization\n",
        "## Define function to add length of comments and or parent comments\n",
        "def add_length_feature_to_dataset(dataset, cols):\n",
        "    for col in cols:\n",
        "        new_col = \"num_\" + col + \"_words\" \n",
        "        dataset[new_col] = dataset[col].apply(lambda x: len(x))\n",
        "    return dataset"
      ],
      "id": "FBdkO8E38t8D",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNnmgVAx8t8D"
      },
      "source": [
        "## Define a function that splits training set into just sarcasm and just non-sarcasm\n",
        "def split_training_dataset_into_separate_labels(training_dataset):\n",
        "    sarcasm = training_dataset[training_dataset['label'] == 1]\n",
        "    non_sarcasm = training_dataset[training_dataset['label'] == 0]\n",
        "    return sarcasm, non_sarcasm\n",
        "\n",
        "## Define function to engineer features for model such as subreddit history and author history\n",
        "def feature_history(training_dataset, col):\n",
        "    history_sarcasm = {}\n",
        "    history_non_sarcasm = {}\n",
        "    \n",
        "    total_comments_by_feature_history = {}\n",
        "    proportion_sarcasm_by_feature_history = {}\n",
        "    \n",
        "    for index, row in training_dataset.iterrows():\n",
        "        if int(row['label']) == 1:\n",
        "            if row[col] not in history_sarcasm:\n",
        "                history_sarcasm[row[col]] = 0\n",
        "                history_non_sarcasm[row[col]] = 0\n",
        "            history_sarcasm[row[col]] += 1\n",
        "    \n",
        "        elif int(row['label']) == 0:\n",
        "            if row[col] not in history_non_sarcasm:\n",
        "                history_non_sarcasm[row[col]] = 0\n",
        "                history_sarcasm[row[col]] = 0\n",
        "            history_non_sarcasm[row[col]] += 1\n",
        "    \n",
        "    for val in history_sarcasm.keys():\n",
        "        num_sarcasm = history_sarcasm[val]\n",
        "        num_non_sarcasm = history_non_sarcasm[val]\n",
        "        total_comments = num_sarcasm + num_non_sarcasm\n",
        "        sarcasm_proportion = num_sarcasm/total_comments\n",
        "        \n",
        "        proportion_sarcasm_by_feature_history[val] = sarcasm_proportion\n",
        "        total_comments_by_feature_history[val] = total_comments\n",
        "    \n",
        "    return proportion_sarcasm_by_feature_history, total_comments_by_feature_history\n",
        "\n",
        "\n",
        "\n",
        "## Define function to prepare training dataset\n",
        "\n",
        "def add_feature_history_to_train(train_dataset, col):\n",
        "    (proportion_history, total_comments_history) = feature_history(train_dataset, col)\n",
        "    proportion_col = \"sarcasm_proportion_by_\" + col\n",
        "    total_col = \"total_num_comments_by_\" + col\n",
        "    \n",
        "    train_dataset[proportion_col] = train_dataset[col].apply(lambda x: proportion_history[x])\n",
        "    train_dataset[total_col] = train_dataset[col].apply(lambda x: total_comments_history[x])\n",
        "    \n",
        "    return train_dataset\n",
        "\n",
        "## Define function to prepare testing dataset\n",
        "\n",
        "def calculate_mean(table):\n",
        "    values = table.values()\n",
        "    return sum(values)/(len(values))\n",
        "\n",
        "def add_feature_history_to_test(test_dataset, col, proportion_history, total_comments_history):\n",
        "    default_proportion = calculate_mean(proportion_history)\n",
        "    default_total_comments = calculate_mean(total_comments_history)\n",
        "    \n",
        "    def getProportion(col_val):\n",
        "        proportion = default_proportion\n",
        "        if col_val in proportion_history:\n",
        "            proportion = proportion_history[col_val]\n",
        "    \n",
        "        return proportion\n",
        "    \n",
        "    def getTotal(col_val):\n",
        "        total = default_total_comments\n",
        "        if col_val in total_comments_history:\n",
        "            total = total_comments_history[col_val]\n",
        "        \n",
        "        return total\n",
        "    \n",
        "    proportion_col = \"sarcasm_proportion_by_\" + col\n",
        "    total_col = \"total_num_comments_by_\" + col\n",
        "    \n",
        "    test_dataset[proportion_col] = test_dataset[col].apply(lambda x: getProportion(x))\n",
        "    test_dataset[total_col] = test_dataset[col].apply(lambda x: getTotal(x))\n",
        "    \n",
        "    return test_dataset"
      ],
      "id": "yNnmgVAx8t8D",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1IRJuFl8t8D"
      },
      "source": [
        "## Before tokenizing\n",
        "## Counting number of exclamation marks\n",
        "def count_num_exclamation_marks(text):\n",
        "    return text.count(\"!\")\n",
        "        \n",
        "def add_num_exclamation_mark_in_feature(dataset, cols):\n",
        "    for col in cols:\n",
        "        dataset[col + \"_num_exclamation_marks\"] = dataset[col].apply(lambda x: count_num_exclamation_marks(x))\n",
        "    return dataset"
      ],
      "id": "W1IRJuFl8t8D",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-HSAVMx8t8E"
      },
      "source": [
        "## Before tokenizing\n",
        "## Counting number of repeated exclamation marks\n",
        "def count_num_repeated_explanation_marks(text):\n",
        "    return text.count(\"!!\")\n",
        "\n",
        "def add_num_repeated_exclamation_mark_in_feature(dataset, cols):\n",
        "    for col in cols:\n",
        "        dataset[col + \"_num_repeated_exclamation_marks\"] = dataset[col].apply(lambda x: count_num_repeated_explanation_marks(x))\n",
        "    return dataset"
      ],
      "id": "s-HSAVMx8t8E",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P71ZikVz8t8E"
      },
      "source": [
        "## Before tokenizing\n",
        "## Count number of emoticons\n",
        "def count_num_common_emoticons(text):\n",
        "    common_emoticons = [\":(\", \":)\", \"<3\", \":'(\", \":')\", \"):\", \"(:\", \"</3\"]\n",
        "    count = 0\n",
        "    for emoticon in common_emoticons:\n",
        "        count += text.count(emoticon)\n",
        "    return count\n",
        "\n",
        "def add_num_emoticons_in_feature(dataset, cols):\n",
        "    for col in cols:\n",
        "        dataset[col + \"_num_emoticons\"] = dataset[col].apply(lambda x: count_num_common_emoticons(x))\n",
        "    return dataset"
      ],
      "id": "P71ZikVz8t8E",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEKX4IEm8t8E"
      },
      "source": [
        "## Before tokenizing\n",
        "## Count number of common \"slang\" style abbreviations\n",
        "def count_num_common_slang(text):\n",
        "    common_slang = [\"kms\", \"smh\", \"smdh\", \"smfh\", \"rofl\", \"roflmao\", \"sic\", \n",
        "                    \"lol\", \"yolo\", \"ikr \", \"dfkm\", \"lmao\", \"ofc\", \"surprise surprise\",\n",
        "                   ]\n",
        "    count = 0\n",
        "    text = text.casefold()\n",
        "    for slang in common_slang:\n",
        "        count += text.count(slang)\n",
        "    return count\n",
        "\n",
        "def add_num_slang_in_feature(dataset, cols):\n",
        "    for col in cols:\n",
        "        dataset[col + \"_num_slang\"] = dataset[col].apply(lambda x: count_num_common_slang(x))\n",
        "    return dataset"
      ],
      "id": "JEKX4IEm8t8E",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACWhoY5S9t3_",
        "outputId": "bad0b1de-075d-4ad4-9f92-c8051ad1f36c"
      },
      "source": [
        "!pip install text2emotion\n",
        "!pip install pyspellchecker"
      ],
      "id": "ACWhoY5S9t3_",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting text2emotion\n",
            "  Downloading text2emotion-0.0.5-py3-none-any.whl (57 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▊                          | 10 kB 27.9 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 20 kB 15.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 30 kB 18.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 40 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 51 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 57 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from text2emotion) (3.2.5)\n",
            "Collecting emoji>=0.6.0\n",
            "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
            "\u001b[?25l\r\u001b[K     |██                              | 10 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 20 kB 43.0 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 30 kB 47.6 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 40 kB 50.7 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 51 kB 53.8 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 61 kB 17.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 71 kB 18.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 81 kB 19.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 92 kB 20.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 102 kB 22.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 112 kB 22.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 122 kB 22.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 133 kB 22.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 143 kB 22.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 153 kB 22.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 163 kB 22.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 170 kB 22.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->text2emotion) (1.15.0)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169314 sha256=5dc62a5517c07d0fd36b810c5ae930c2833add7325e062c373697fbd938e9e19\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/5f/d3/03d313ddb3c2a1a427bb4690f1621eea60fe6f2a30cc95940f\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, text2emotion\n",
            "Successfully installed emoji-1.6.1 text2emotion-0.0.5\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.6.2-py3-none-any.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 13.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evrKPAlO8t8E",
        "outputId": "6a609e4b-b541-4e77-9f11-eb5545907a16"
      },
      "source": [
        "## Before tokenizing\n",
        "## Measure emotions of text\n",
        "import text2emotion as t2e\n",
        "def get_emotions_from_text(text):\n",
        "    return t2e.get_emotion(text)\n",
        "\n",
        "def get_emotion_from_text(text, emotion):\n",
        "    return get_emotions_from_text(text)[emotion]\n",
        "\n",
        "\n",
        "\n",
        "def add_emotions_features_to_dataset(dataset, cols, emotions):\n",
        "    for col in cols:\n",
        "        for emotion in emotions:\n",
        "            col_name = col + \"_\" + emotion\n",
        "            dataset[col_name] = dataset[col].apply(lambda x: get_emotion_from_text(x, emotion))\n",
        "    return dataset\n",
        "\n",
        "print(get_emotions_from_text(\"I love you\"))\n",
        "print(get_emotion_from_text(\"I love you\", \"Happy\"))"
      ],
      "id": "evrKPAlO8t8E",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "{'Happy': 1.0, 'Angry': 0.0, 'Surprise': 0.0, 'Sad': 0.0, 'Fear': 0.0}\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KTikx3g8t8E"
      },
      "source": [
        "## After tokenizing\n",
        "## Count number of misspelled words\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "spellchecker = SpellChecker(language=\"en\")\n",
        "\n",
        "def count_number_of_misspelled_words(text):\n",
        "    count = 0\n",
        "    misspelled_words = spellchecker.unknown(text)\n",
        "    return len(misspelled_words)\n",
        "\n",
        "def add_num_misspelled_words_feature(dataset, cols):\n",
        "    for col in cols:\n",
        "        dataset[col + \"_num_misspelled_words\"] = dataset[col].apply(lambda x: count_number_of_misspelled_words(x))\n",
        "    return dataset"
      ],
      "id": "2KTikx3g8t8E",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_0ZIAq08t8E"
      },
      "source": [
        "## After tokenizing\n",
        "## Measure misspelling in a different way - by summing up edit distances\n",
        "from nltk.metrics import edit_distance\n",
        "\n",
        "def measure_sum_of_edit_distances(text):\n",
        "    distances = 0\n",
        "    misspelled_words = spellchecker.unknown(text)\n",
        "    for misspelled_word in misspelled_words:\n",
        "        corrected_word = spellchecker.correction(misspelled_word)\n",
        "        distances += edit_distance(corrected_word, misspelled_word)\n",
        "    return distances\n",
        "\n",
        "def add_sum_of_edit_distances_feature(dataset, cols):\n",
        "    for col in cols:\n",
        "        dataset[col + \"_edit_distance_misspelled_words\"] = dataset[col].apply(lambda x: measure_sum_of_edit_distances(x))\n",
        "    return dataset"
      ],
      "id": "9_0ZIAq08t8E",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CusguQDd4cMh"
      },
      "source": [
        "## Prepare Data Structures"
      ],
      "id": "CusguQDd4cMh"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PocpP_Z_8t8F"
      },
      "source": [
        "## Load CSV\n",
        "csv_feature_engineering = pd.read_csv(\"train-balanced-sarcasm.csv\")\n",
        "csv_feature_engineering.dropna(subset=['comment', 'parent_comment'], inplace=True)\n",
        "csv_feature_engineering[\"comment\"] = csv_feature_engineering[\"comment\"].astype(str)\n",
        "csv_feature_engineering[\"parent_comment\"] = csv_feature_engineering[\"parent_comment\"].astype(str)"
      ],
      "id": "PocpP_Z_8t8F",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94gEP2YI8t8F"
      },
      "source": [
        "labels = csv_feature_engineering['label']\n",
        "training_csv_feature_engineering, testing_csv_feature_engineering, label_train, label_test = train_test_split(csv_feature_engineering, labels, test_size=0.50, random_state=1000)\n"
      ],
      "id": "94gEP2YI8t8F",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVaoORuv8t8F"
      },
      "source": [
        "COMMENT_AND_PARENT_COMMENT = [\"comment\", \"parent_comment\"]\n",
        "COMMENT = [\"comment\"]\n",
        "PARENT_COMMENT = [\"parent_comment\"]\n",
        "AUTHOR = \"author\"\n",
        "SUBREDDIT = \"subreddit\"\n",
        "EMOTIONS = [\"Happy\", \"Sad\", \"Angry\", \"Surprise\", \"Fear\"]"
      ],
      "id": "FVaoORuv8t8F",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ynp5EtRx4p8_"
      },
      "source": [
        "## Add Features Before Tokenization"
      ],
      "id": "Ynp5EtRx4p8_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXuUYpB98t8F",
        "outputId": "8b849550-9d87-428e-a320-e0b36cbb9b0f"
      },
      "source": [
        "## Add BEFORE tokenization features to training data\n",
        "training_csv_feature_engineering = add_num_exclamation_mark_in_feature(\n",
        "    training_csv_feature_engineering, COMMENT_AND_PARENT_COMMENT)\n",
        "\n",
        "training_csv_feature_engineering = add_num_repeated_exclamation_mark_in_feature(\n",
        "    training_csv_feature_engineering, COMMENT_AND_PARENT_COMMENT)\n",
        "\n",
        "training_csv_feature_engineering = add_num_emoticons_in_feature(\n",
        "    training_csv_feature_engineering, COMMENT_AND_PARENT_COMMENT)\n",
        "\n",
        "training_csv_feature_engineering = add_num_slang_in_feature(\n",
        "    training_csv_feature_engineering, COMMENT_AND_PARENT_COMMENT)\n",
        "\n",
        "## training_csv_feature_engineering = add_emotions_features_to_dataset(\n",
        "##    training_csv_feature_engineering, COMMENT_AND_PARENT_COMMENT, EMOTIONS)"
      ],
      "id": "fXuUYpB98t8F",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLdRZ9B08t8F",
        "outputId": "11e5d9db-1fdd-418f-e431-b5089ad37a7f"
      },
      "source": [
        "## Add BEFORE tokenization features to testing data\n",
        "testing_csv_feature_engineering = add_num_exclamation_mark_in_feature(\n",
        "    testing_csv_feature_engineering, COMMENT_AND_PARENT_COMMENT)\n",
        "\n",
        "testing_csv_feature_engineering = add_num_repeated_exclamation_mark_in_feature(\n",
        "    testing_csv_feature_engineering, COMMENT_AND_PARENT_COMMENT)\n",
        "\n",
        "testing_csv_feature_engineering = add_num_emoticons_in_feature(\n",
        "    testing_csv_feature_engineering, COMMENT_AND_PARENT_COMMENT)\n",
        "\n",
        "testing_csv_feature_engineering = add_num_slang_in_feature(\n",
        "    testing_csv_feature_engineering, COMMENT_AND_PARENT_COMMENT)\n",
        "\n",
        "## testing_csv_feature_engineering = add_emotions_features_to_dataset(\n",
        "##    testing_csv_feature_engineering, COMMENT_AND_PARENT_COMMENT, EMOTIONS)"
      ],
      "id": "uLdRZ9B08t8F",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa_pU0iN4sas"
      },
      "source": [
        "## Tokenize data"
      ],
      "id": "Pa_pU0iN4sas"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hySP4fc8t8F",
        "outputId": "6e47a54b-79c3-4d81-c693-c70b99e39606"
      },
      "source": [
        "## tokenize training data\n",
        "training_csv_feature_engineering = remove_stopwords_and_tokenize_cols_in_dataset(\n",
        "    training_csv_feature_engineering, COMMENT_AND_PARENT_COMMENT)"
      ],
      "id": "8hySP4fc8t8F",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH9rez318t8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33a6eea1-ff68-4954-9bcb-26a73839f2a3"
      },
      "source": [
        "## tokenize testing data\n",
        "testing_csv_feature_engineering = remove_stopwords_and_tokenize_cols_in_dataset(\n",
        "    testing_csv_feature_engineering, COMMENT_AND_PARENT_COMMENT)"
      ],
      "id": "dH9rez318t8F",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLA5IY5w4u_N"
      },
      "source": [
        "## Add Features After Tokenization"
      ],
      "id": "pLA5IY5w4u_N"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npzBxiWQ8t8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "767fc169-b3e8-410f-ac1b-85269fdda3b5"
      },
      "source": [
        "## Add AFTER tokenization features to training data\n",
        "training_csv_feature_engineering = add_length_feature_to_dataset(\n",
        "    training_csv_feature_engineering, COMMENT_AND_PARENT_COMMENT)\n",
        "\n",
        "training_csv_feature_engineering = add_feature_history_to_train(\n",
        "    training_csv_feature_engineering, AUTHOR)\n",
        "\n",
        "training_csv_feature_engineering = add_feature_history_to_train(\n",
        "    training_csv_feature_engineering, SUBREDDIT)\n",
        "\n",
        "training_csv_feature_engineering = add_num_misspelled_words_feature(\n",
        "    training_csv_feature_engineering, COMMENT_AND_PARENT_COMMENT)\n",
        "\n",
        "## training_csv_feature_engineering = add_sum_of_edit_distances_feature(\n",
        "    ## training_csv_feature_engineering, COMMENT_AND_PARENT_COMMENT)"
      ],
      "id": "npzBxiWQ8t8G",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB8gPHba8t8G"
      },
      "source": [
        "proportion_history_author, total_comments_history_author = feature_history(\n",
        "    training_csv_feature_engineering, AUTHOR)\n",
        "proportion_history_subreddit, total_comments_history_subreddit = feature_history(\n",
        "    training_csv_feature_engineering, SUBREDDIT)"
      ],
      "id": "dB8gPHba8t8G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1ICMjpo8t8G"
      },
      "source": [
        "## Add AFTER tokenization features to testing data\n",
        "testing_csv_feature_engineering = add_length_feature_to_dataset(\n",
        "    testing_csv_feature_engineering, COMMENT_AND_PARENT_COMMENT)\n",
        "\n",
        "testing_csv_feature_engineering = add_feature_history_to_test(\n",
        "    testing_csv_feature_engineering, AUTHOR, \n",
        "    proportion_history_author, total_comments_history_author)\n",
        "\n",
        "testing_csv_feature_engineering = add_feature_history_to_test(\n",
        "    testing_csv_feature_engineering, SUBREDDIT,\n",
        "    proportion_history_subreddit, total_comments_history_subreddit)\n",
        "\n",
        "testing_csv_feature_engineering = add_num_misspelled_words_feature(\n",
        "    testing_csv_feature_engineering, COMMENT_AND_PARENT_COMMENT)\n",
        "\n",
        "## testing_csv_feature_engineering = add_sum_of_edit_distances_feature(\n",
        "    ## testing_csv_feature_engineering, COMMENT_AND_PARENT_COMMENT)"
      ],
      "id": "g1ICMjpo8t8G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ykmhymk_AEE"
      },
      "source": [
        "## Normalize Data"
      ],
      "id": "7ykmhymk_AEE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "id": "8wJ4PhBKxjEN",
        "outputId": "90412e2b-b02b-4664-e0b2-89d0395f5166"
      },
      "source": [
        "from google.colab import files\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler() \n",
        "scaled_values = scaler.fit_transform(training_csv_feature_engineering.iloc[:, 10:])\n",
        "training_csv_feature_engineering.iloc[:,10:] = scaled_values\n",
        "\n",
        "scaled_values = scaler.fit_transform(testing_csv_feature_engineering.iloc[:, 10:]) \n",
        "testing_csv_feature_engineering.iloc[:,10:] = scaled_values"
      ],
      "id": "8wJ4PhBKxjEN",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1734: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(loc, value[:, i].tolist())\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_517f97e0-02a3-4a0f-aa7a-2f73d353413a\", \"testing_data_engineered.csv\", 3293294)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RmBlBHD_Kvs"
      },
      "source": [
        "## Download files in case"
      ],
      "id": "0RmBlBHD_Kvs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_HlYu-N-9C_"
      },
      "source": [
        "training_csv_feature_engineering.to_csv(r'training_data_engineered.csv', index = False, header=True)\n",
        "files.download(\"training_data_engineered.csv\")\n",
        "\n",
        "testing_csv_feature_engineering.to_csv(r'testing_data_engineered.csv', index = False, header=True)\n",
        "files.download(\"testing_data_engineered.csv\")"
      ],
      "id": "z_HlYu-N-9C_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmMhQN9Q_PHu"
      },
      "source": [
        "## Data Preview"
      ],
      "id": "lmMhQN9Q_PHu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1Z94Z_38t8G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "69a92827-ff66-41b6-a764-f720a93e623e"
      },
      "source": [
        "## Preview of features\n",
        "## Engineered features from column 10 to end (0 based indexing)\n",
        "training_csv_feature_engineering.head()"
      ],
      "id": "K1Z94Z_38t8G",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>comment</th>\n",
              "      <th>author</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>score</th>\n",
              "      <th>ups</th>\n",
              "      <th>downs</th>\n",
              "      <th>date</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>parent_comment</th>\n",
              "      <th>comment_num_exclamation_marks</th>\n",
              "      <th>parent_comment_num_exclamation_marks</th>\n",
              "      <th>comment_num_repeated_exclamation_marks</th>\n",
              "      <th>parent_comment_num_repeated_exclamation_marks</th>\n",
              "      <th>comment_num_emoticons</th>\n",
              "      <th>parent_comment_num_emoticons</th>\n",
              "      <th>comment_num_slang</th>\n",
              "      <th>parent_comment_num_slang</th>\n",
              "      <th>num_comment_words</th>\n",
              "      <th>num_parent_comment_words</th>\n",
              "      <th>sarcasm_proportion_by_author</th>\n",
              "      <th>total_num_comments_by_author</th>\n",
              "      <th>sarcasm_proportion_by_subreddit</th>\n",
              "      <th>total_num_comments_by_subreddit</th>\n",
              "      <th>comment_num_misspelled_words</th>\n",
              "      <th>parent_comment_num_misspelled_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1566</th>\n",
              "      <td>0</td>\n",
              "      <td>[A, sociologist, evaluating, people, personali...</td>\n",
              "      <td>Blood_magic</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>2016-11</td>\n",
              "      <td>2016-11-03 20:52:50</td>\n",
              "      <td>[Psychopath-, person, suffering, chronic, ment...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004548</td>\n",
              "      <td>0.020238</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.307263</td>\n",
              "      <td>0.993060</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.066667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22455</th>\n",
              "      <td>1</td>\n",
              "      <td>[Like, one, rideau, street]</td>\n",
              "      <td>rbooris</td>\n",
              "      <td>ottawa</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>2016-10</td>\n",
              "      <td>2016-10-17 23:21:23</td>\n",
              "      <td>[Montreal, less, three, times, bigger, Ottawa,...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002599</td>\n",
              "      <td>0.019643</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.004858</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.033333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5118</th>\n",
              "      <td>0</td>\n",
              "      <td>[So, make, .]</td>\n",
              "      <td>desicriger_KS</td>\n",
              "      <td>kulchasimulator</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>2016-12</td>\n",
              "      <td>2016-12-07 20:14:38</td>\n",
              "      <td>[Sweet, potatoes, :, The, fuck, Value, !]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001949</td>\n",
              "      <td>0.004167</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6352</th>\n",
              "      <td>0</td>\n",
              "      <td>[Type, :, Null, might, failed, attempt, create...</td>\n",
              "      <td>Dragon_Fang</td>\n",
              "      <td>pokemon</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2016-09</td>\n",
              "      <td>2016-09-08 17:17:58</td>\n",
              "      <td>[``, Type, :, All, '', Arceus]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.007147</td>\n",
              "      <td>0.003571</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>0.060375</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.033333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1992</th>\n",
              "      <td>0</td>\n",
              "      <td>[Based, centipede, !]</td>\n",
              "      <td>ShootUpPot</td>\n",
              "      <td>formula1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2016-09</td>\n",
              "      <td>2016-09-12 22:03:57</td>\n",
              "      <td>[Bernie, gets, based, every, day., I, 'm, goin...</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001949</td>\n",
              "      <td>0.006548</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.049271</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.022222</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       label  ... parent_comment_num_misspelled_words\n",
              "1566       0  ...                            0.066667\n",
              "22455      1  ...                            0.033333\n",
              "5118       0  ...                            0.000000\n",
              "6352       0  ...                            0.033333\n",
              "1992       0  ...                            0.022222\n",
              "\n",
              "[5 rows x 26 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4B_m5cz8t8G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "d50e5f84-8b61-45c8-a8c9-f6d359cc18df"
      },
      "source": [
        "## Preview of features\n",
        "## Engineered features from column 10 to end (0 based indexing)\n",
        "testing_csv_feature_engineering.head()"
      ],
      "id": "R4B_m5cz8t8G",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>comment</th>\n",
              "      <th>author</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>score</th>\n",
              "      <th>ups</th>\n",
              "      <th>downs</th>\n",
              "      <th>date</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>parent_comment</th>\n",
              "      <th>comment_num_exclamation_marks</th>\n",
              "      <th>parent_comment_num_exclamation_marks</th>\n",
              "      <th>comment_num_repeated_exclamation_marks</th>\n",
              "      <th>parent_comment_num_repeated_exclamation_marks</th>\n",
              "      <th>comment_num_emoticons</th>\n",
              "      <th>parent_comment_num_emoticons</th>\n",
              "      <th>comment_num_slang</th>\n",
              "      <th>parent_comment_num_slang</th>\n",
              "      <th>num_comment_words</th>\n",
              "      <th>num_parent_comment_words</th>\n",
              "      <th>sarcasm_proportion_by_author</th>\n",
              "      <th>total_num_comments_by_author</th>\n",
              "      <th>sarcasm_proportion_by_subreddit</th>\n",
              "      <th>total_num_comments_by_subreddit</th>\n",
              "      <th>comment_num_misspelled_words</th>\n",
              "      <th>parent_comment_num_misspelled_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18352</th>\n",
              "      <td>1</td>\n",
              "      <td>[Needs, fenders, touring, rack, .]</td>\n",
              "      <td>cbleslie</td>\n",
              "      <td>Bikeporn</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>2016-12</td>\n",
              "      <td>2016-12-09 21:26:32</td>\n",
              "      <td>[Specialized, Shiv, (, comments, )]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.032051</td>\n",
              "      <td>0.006916</td>\n",
              "      <td>0.382585</td>\n",
              "      <td>0.008075</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.000694</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23480</th>\n",
              "      <td>0</td>\n",
              "      <td>[Why, would, want, ?]</td>\n",
              "      <td>teyxen</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>2016-11</td>\n",
              "      <td>2016-11-09 05:12:51</td>\n",
              "      <td>[I, cant, access, r/4chan, ,, What, happened, ?]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.025641</td>\n",
              "      <td>0.011065</td>\n",
              "      <td>0.382585</td>\n",
              "      <td>0.008075</td>\n",
              "      <td>0.307263</td>\n",
              "      <td>0.993060</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.016949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26321</th>\n",
              "      <td>0</td>\n",
              "      <td>[This, exactly, thoughts, wasnt, sure, word, ,...</td>\n",
              "      <td>lakie92</td>\n",
              "      <td>ffxiv</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>2016-11</td>\n",
              "      <td>2016-11-12 12:46:57</td>\n",
              "      <td>[Just, add, different, opinion, ,, believe, gu...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.121795</td>\n",
              "      <td>0.136929</td>\n",
              "      <td>0.382585</td>\n",
              "      <td>0.008075</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.016655</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.169492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9817</th>\n",
              "      <td>1</td>\n",
              "      <td>[In, creased, surface, area, better, cooling, ...</td>\n",
              "      <td>BuzzfeedPersonified</td>\n",
              "      <td>Justrolledintotheshop</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2016-09</td>\n",
              "      <td>2016-09-22 11:34:03</td>\n",
              "      <td>[It, 's, super, special, ``, textured, brake, ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.051282</td>\n",
              "      <td>0.013831</td>\n",
              "      <td>0.382585</td>\n",
              "      <td>0.008075</td>\n",
              "      <td>0.521739</td>\n",
              "      <td>0.015267</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.067797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26916</th>\n",
              "      <td>1</td>\n",
              "      <td>[But, walked, dispensary, Seattle, suspended]</td>\n",
              "      <td>sathyyyy</td>\n",
              "      <td>nfl</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>2016-11</td>\n",
              "      <td>2016-11-13 17:43:57</td>\n",
              "      <td>[I, get, ,, really, seems, like, 's, nothing, .]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.032051</td>\n",
              "      <td>0.012448</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.410714</td>\n",
              "      <td>0.232477</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.016949</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       label  ... parent_comment_num_misspelled_words\n",
              "18352      1  ...                            0.000000\n",
              "23480      0  ...                            0.016949\n",
              "26321      0  ...                            0.169492\n",
              "9817       1  ...                            0.067797\n",
              "26916      1  ...                            0.016949\n",
              "\n",
              "[5 rows x 26 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZVbQqkp8t8G"
      },
      "source": [
        "## for downloading df into csv for R visualisation\n",
        "## training_csv_feature_engineering.to_csv (r'FOLDER_PATH\\FILE_NAME.csv', index = False, header=True)"
      ],
      "id": "cZVbQqkp8t8G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAMT17Cl8t8G"
      },
      "source": [
        "## Baseline model (Logistic Regression)"
      ],
      "id": "SAMT17Cl8t8G"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSHlROK38t8H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "444a0220-9c14-4629-f89b-38d19fa650a0"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "## Without word embeddings and word vectorization\n",
        "X_train = training_csv_feature_engineering.iloc[:,10:]\n",
        "X_test = testing_csv_feature_engineering.iloc[:,10:]\n",
        "\n",
        "LogisticRegression_classifier = LogisticRegression()\n",
        "LogisticRegression_classifier.fit(X_train, label_train)\n",
        "score = LogisticRegression_classifier.score(X_test, label_test)\n",
        "print(score)"
      ],
      "id": "FSHlROK38t8H",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5753330586457904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEVX9KDv-cKN"
      },
      "source": [
        ""
      ],
      "id": "TEVX9KDv-cKN",
      "execution_count": null,
      "outputs": []
    }
  ]
}