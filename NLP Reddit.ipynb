{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3436568",
   "metadata": {},
   "source": [
    "## Reddit Sarcasm Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2026e9b0",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd6f091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0fbc20",
   "metadata": {},
   "source": [
    "### Import CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56714857",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_csv_1 = pd.read_csv(\"train-balanced-sarcasm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e75419dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_csv_1[\"comment\"] = training_csv_1[\"comment\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffa5dd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NC and NH.</td>\n",
       "      <td>Trumpbart</td>\n",
       "      <td>politics</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-16 23:55:23</td>\n",
       "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>You do know west teams play against west teams...</td>\n",
       "      <td>Shbshb906</td>\n",
       "      <td>nba</td>\n",
       "      <td>-4</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-11</td>\n",
       "      <td>2016-11-01 00:24:10</td>\n",
       "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>They were underdogs earlier today, but since G...</td>\n",
       "      <td>Creepeth</td>\n",
       "      <td>nfl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-22 21:45:37</td>\n",
       "      <td>They're favored to win.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
       "      <td>icebrotha</td>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>2016-10-18 21:03:47</td>\n",
       "      <td>deadass don't kill my buzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I could use one of those tools.</td>\n",
       "      <td>cush2push</td>\n",
       "      <td>MaddenUltimateTeam</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>2016-12-30 17:00:13</td>\n",
       "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            comment     author  \\\n",
       "0      0                                         NC and NH.  Trumpbart   \n",
       "1      0  You do know west teams play against west teams...  Shbshb906   \n",
       "2      0  They were underdogs earlier today, but since G...   Creepeth   \n",
       "3      0  This meme isn't funny none of the \"new york ni...  icebrotha   \n",
       "4      0                    I could use one of those tools.  cush2push   \n",
       "\n",
       "            subreddit  score  ups  downs     date          created_utc  \\\n",
       "0            politics      2   -1     -1  2016-10  2016-10-16 23:55:23   \n",
       "1                 nba     -4   -1     -1  2016-11  2016-11-01 00:24:10   \n",
       "2                 nfl      3    3      0  2016-09  2016-09-22 21:45:37   \n",
       "3  BlackPeopleTwitter     -8   -1     -1  2016-10  2016-10-18 21:03:47   \n",
       "4  MaddenUltimateTeam      6   -1     -1  2016-12  2016-12-30 17:00:13   \n",
       "\n",
       "                                      parent_comment  \n",
       "0  Yeah, I get that argument. At this point, I'd ...  \n",
       "1  The blazers and Mavericks (The wests 5 and 6 s...  \n",
       "2                            They're favored to win.  \n",
       "3                         deadass don't kill my buzz  \n",
       "4  Yep can confirm I saw the tool they use for th...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_csv_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe97033",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b446461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total training data has 256561 rows.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.500000    251903\n",
       "1.000000      2302\n",
       "0.333333       770\n",
       "0.400000       405\n",
       "0.428571       268\n",
       "             ...  \n",
       "0.533333         1\n",
       "0.494253         1\n",
       "0.555556         1\n",
       "0.499408         1\n",
       "0.495050         1\n",
       "Name: label, Length: 64, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"The total training data has {training_csv_1.author.nunique()} rows.\")\n",
    "training_csv_1.groupby(\"author\").mean()[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40dc3a6",
   "metadata": {},
   "source": [
    "##### The authors is mostly 0.5 probability of each label, might consider dropping it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "870eb676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total training data has 14878 rows.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.000000    5883\n",
       "1.000000    2042\n",
       "0.500000    1242\n",
       "0.333333     585\n",
       "0.250000     362\n",
       "            ... \n",
       "0.465517       1\n",
       "0.506567       1\n",
       "0.257874       1\n",
       "0.550802       1\n",
       "0.373333       1\n",
       "Name: label, Length: 1430, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"The total training data has {training_csv_1.subreddit.nunique()} rows.\")\n",
    "training_csv_1.groupby(\"subreddit\").mean()[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a086d8d0",
   "metadata": {},
   "source": [
    "##### Subreddit seems to provide more info than expected, should probably keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3304002a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010821</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010822</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010823</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010824</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010825</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1010826 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ups  downs\n",
       "0         -1     -1\n",
       "1         -1     -1\n",
       "2          3      0\n",
       "3         -1     -1\n",
       "4         -1     -1\n",
       "...      ...    ...\n",
       "1010821    2      0\n",
       "1010822    1      0\n",
       "1010823    1      0\n",
       "1010824    1      0\n",
       "1010825    2      0\n",
       "\n",
       "[1010826 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_csv_1[[\"ups\", \"downs\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94ef3be",
   "metadata": {},
   "source": [
    "##### Notice how ups and downs seem to have a correlation? Lets test this theory out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a7de7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>comment</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>date</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>parent_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0</td>\n",
       "      <td>My comment very similar to this went down a fu...</td>\n",
       "      <td>Schumarker</td>\n",
       "      <td>Android</td>\n",
       "      <td>-6</td>\n",
       "      <td>-6</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-24 21:50:56</td>\n",
       "      <td>Badumm-tzz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0</td>\n",
       "      <td>it really does</td>\n",
       "      <td>Horus_Krishna_2</td>\n",
       "      <td>radiohead</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-14 20:07:04</td>\n",
       "      <td>As far as I know, someone's reddit history doe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>0</td>\n",
       "      <td>Meh, my upper body blows his away.</td>\n",
       "      <td>GiveMeSomeIhedigbo</td>\n",
       "      <td>bodybuilding</td>\n",
       "      <td>-6</td>\n",
       "      <td>-6</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-19 06:27:32</td>\n",
       "      <td>Do you Agree that this version is The BEST Ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>0</td>\n",
       "      <td>Such a shitty meme.</td>\n",
       "      <td>Geralt-of_Rivia</td>\n",
       "      <td>AdviceAnimals</td>\n",
       "      <td>-4</td>\n",
       "      <td>-4</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-02 02:39:44</td>\n",
       "      <td>Front page post with 2000 comments and is 10 h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>0</td>\n",
       "      <td>This sub is for open ended questions, not yes ...</td>\n",
       "      <td>hunterz5</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>-3</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>2016-09-10 01:54:47</td>\n",
       "      <td>Do you think IB/AP classes are truly worth it?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010772</th>\n",
       "      <td>1</td>\n",
       "      <td>NSFW, thanks.</td>\n",
       "      <td>Underdogg13</td>\n",
       "      <td>pics</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-09</td>\n",
       "      <td>2009-09-06 17:16:11</td>\n",
       "      <td>(PIC) Penelope Cruz, firm and without shirt. F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010790</th>\n",
       "      <td>1</td>\n",
       "      <td>.. erm .. good for them, i guess .. they're ma...</td>\n",
       "      <td>mijj</td>\n",
       "      <td>Economics</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-10</td>\n",
       "      <td>2009-10-30 01:12:47</td>\n",
       "      <td>There was a rash of muggings in my neighborhoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010791</th>\n",
       "      <td>1</td>\n",
       "      <td>zombie, frankenstein, jesus, now thats a real ...</td>\n",
       "      <td>Rip_Van_Winkle</td>\n",
       "      <td>pics</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-10</td>\n",
       "      <td>2009-10-31 23:20:10</td>\n",
       "      <td>jesus christ, that's a funny diagram.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010801</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes, and there's no such thing as mental illne...</td>\n",
       "      <td>Davin900</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-08</td>\n",
       "      <td>2009-08-14 18:34:29</td>\n",
       "      <td>And my parents had a rough upbringing/backgrou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010815</th>\n",
       "      <td>0</td>\n",
       "      <td>Who said I didn't have a big dick?</td>\n",
       "      <td>jemenfiche</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>2009-02</td>\n",
       "      <td>2009-02-20 02:27:15</td>\n",
       "      <td>Some of us have big dicks.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61730 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                            comment  \\\n",
       "140          0  My comment very similar to this went down a fu...   \n",
       "204          0                                     it really does   \n",
       "414          0                 Meh, my upper body blows his away.   \n",
       "431          0                                Such a shitty meme.   \n",
       "454          0  This sub is for open ended questions, not yes ...   \n",
       "...        ...                                                ...   \n",
       "1010772      1                                      NSFW, thanks.   \n",
       "1010790      1  .. erm .. good for them, i guess .. they're ma...   \n",
       "1010791      1  zombie, frankenstein, jesus, now thats a real ...   \n",
       "1010801      1  Yes, and there's no such thing as mental illne...   \n",
       "1010815      0                 Who said I didn't have a big dick?   \n",
       "\n",
       "                     author      subreddit  score  ups  downs     date  \\\n",
       "140              Schumarker        Android     -6   -6      0  2016-09   \n",
       "204         Horus_Krishna_2      radiohead     -1   -1      0  2016-09   \n",
       "414      GiveMeSomeIhedigbo   bodybuilding     -6   -6      0  2016-09   \n",
       "431         Geralt-of_Rivia  AdviceAnimals     -4   -4      0  2016-09   \n",
       "454                hunterz5      AskReddit     -3   -3      0  2016-09   \n",
       "...                     ...            ...    ...  ...    ...      ...   \n",
       "1010772         Underdogg13           pics     -1   -1      0  2009-09   \n",
       "1010790                mijj      Economics     -2   -2      0  2009-10   \n",
       "1010791      Rip_Van_Winkle           pics     -2   -2      0  2009-10   \n",
       "1010801            Davin900      worldnews     -1   -1      0  2009-08   \n",
       "1010815          jemenfiche     reddit.com     -2   -2      0  2009-02   \n",
       "\n",
       "                 created_utc  \\\n",
       "140      2016-09-24 21:50:56   \n",
       "204      2016-09-14 20:07:04   \n",
       "414      2016-09-19 06:27:32   \n",
       "431      2016-09-02 02:39:44   \n",
       "454      2016-09-10 01:54:47   \n",
       "...                      ...   \n",
       "1010772  2009-09-06 17:16:11   \n",
       "1010790  2009-10-30 01:12:47   \n",
       "1010791  2009-10-31 23:20:10   \n",
       "1010801  2009-08-14 18:34:29   \n",
       "1010815  2009-02-20 02:27:15   \n",
       "\n",
       "                                            parent_comment  \n",
       "140                                             Badumm-tzz  \n",
       "204      As far as I know, someone's reddit history doe...  \n",
       "414      Do you Agree that this version is The BEST Ver...  \n",
       "431      Front page post with 2000 comments and is 10 h...  \n",
       "454      Do you think IB/AP classes are truly worth it?...  \n",
       "...                                                    ...  \n",
       "1010772  (PIC) Penelope Cruz, firm and without shirt. F...  \n",
       "1010790  There was a rash of muggings in my neighborhoo...  \n",
       "1010791              jesus christ, that's a funny diagram.  \n",
       "1010801  And my parents had a rough upbringing/backgrou...  \n",
       "1010815                         Some of us have big dicks.  \n",
       "\n",
       "[61730 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_csv_1[training_csv_1[\"ups\"].apply(lambda x: -1 if x <= -1 else 0) != training_csv_1[\"downs\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4728af8d",
   "metadata": {},
   "source": [
    "##### Only 6.1% does not follow the rules, is downs worth keeping? Debatable I guess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57271aa7",
   "metadata": {},
   "source": [
    "### Build model using Comment Column only (Unigram Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65712b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "789a9be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c760888",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Better abstraction\n",
    "\n",
    "class sklearnClassifier:\n",
    "    def __init__(self, model, data, label, fitBool = True):\n",
    "        self.model = model\n",
    "        if fitBool: self.fit(data, label)\n",
    "            \n",
    "    def fit(self, data, label):\n",
    "        self.model.fit(data, label)\n",
    "    \n",
    "    def score(self, X, y_true):\n",
    "        y_pred = self.model.predict(X)\n",
    "        print(f\"Accuracy score: {accuracy_score(y_true, y_pred)}\")\n",
    "        print(f\"Recall score: {recall_score(y_true, y_pred)}\")\n",
    "        print(f\"Precision score: {precision_score(y_true, y_pred)}\")\n",
    "        print(f\"F1 score: {f1_score(y_true, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e459e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_csv_1[\"comment\"] = training_csv_1[\"comment\"].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b4a75b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    training_csv_1[\"comment\"], \n",
    "    training_csv_1[\"label\"], \n",
    "    test_size = 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3c11c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram_vectorizer(text_train, ngram_range = (1,1), **kwargs):\n",
    "    vectorizer = CountVectorizer(ngram_range = ngram_range, **kwargs)\n",
    "    vectorizer.fit(text_train)\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5189813",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_vectorizer = create_ngram_vectorizer(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9e72724",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = unigram_vectorizer.transform(X_train)\n",
    "X_val_transformed = unigram_vectorizer.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c842c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_classifier = sklearnClassifier(SGDClassifier(), X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "120b0aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: \n",
      "Accuracy score: 0.6890992506121237\n",
      "Recall score: 0.5690355668866672\n",
      "Precision score: 0.7489519866687496\n",
      "F1 score: 0.6467137222279211\n",
      "Validation: \n",
      "Accuracy score: 0.681791201290029\n",
      "Recall score: 0.5605424668382498\n",
      "Precision score: 0.7396000679179238\n",
      "F1 score: 0.6377412252298924\n"
     ]
    }
   ],
   "source": [
    "print(\"Training: \")\n",
    "base_classifier.score(X_train_transformed, y_train)\n",
    "print(\"Validation: \")\n",
    "base_classifier.score(X_val_transformed, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7574c404",
   "metadata": {},
   "source": [
    "### Now what? Bigrams and Trigrams, LETZ GO!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12802ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1, 3): # Trigram is a bit slow so we'll bring that back later\n",
    "#     igram_vectorizer = create_ngram_vectorizer(X_train, ngram_range = (1,i))\n",
    "#     X_train_transformed = igram_vectorizer.transform(X_train)\n",
    "#     X_val_transformed = igram_vectorizer.transform(X_val)\n",
    "    \n",
    "#     base_classifier = sklearnClassifier(SGDClassifier(), X_train_transformed, y_train)\n",
    "    \n",
    "#     print(\"Training: \")\n",
    "#     base_classifier.score(X_train_transformed, y_train)\n",
    "#     print(\"Validation: \")\n",
    "#     base_classifier.score(X_val_transformed, y_val)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4b4158",
   "metadata": {},
   "source": [
    "### Using TFIDF instead of just counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8f9d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e22e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_ngram_vectorizer(text_train, ngram_range = (1,1), **kwargs):\n",
    "    vectorizer = TfidfVectorizer(ngram_range = ngram_range, **kwargs)\n",
    "    vectorizer.fit(text_train)\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f37f92e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i in range(1,3):\n",
    "#     tfidf_igram_vectorizer = create_tfidf_ngram_vectorizer(X_train, ngram_range = (1,i))\n",
    "#     X_train_transformed = tfidf_igram_vectorizer.transform(X_train)\n",
    "#     X_val_transformed = tfidf_igram_vectorizer.transform(X_val)\n",
    "    \n",
    "#     base_classifier = sklearnClassifier(SGDClassifier(), X_train_transformed, y_train)\n",
    "    \n",
    "#     print(\"Training: \")\n",
    "#     base_classifier.score(X_train_transformed, y_train)\n",
    "#     print(\"Validation: \")\n",
    "#     base_classifier.score(X_val_transformed, y_val)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60945fbe",
   "metadata": {},
   "source": [
    "### Vector Representation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76cc1ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Abstraction for easier work\n",
    "class EmbeddingTechniques:\n",
    "    def __init__(self, method):\n",
    "        self.transformMethod = method\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.transformMethod(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "795c3b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingTester:\n",
    "    def __init__(self, sklearnmodel):\n",
    "        self.list_of_techniques = {}\n",
    "        self.tokenized = {}\n",
    "        self.model = sklearnmodel\n",
    "        \n",
    "    def addEmbeddingTechniques(self, key, method, tokenized = False):\n",
    "        self.list_of_techniques[key] = method\n",
    "        self.tokenized[key] = tokenized\n",
    "        \n",
    "        \n",
    "    def testModel(self, X_train_transformed, y_train_true, X_test_transformed, y_test_true, text = None):\n",
    "        if text is not None: print(text)\n",
    "        self.model.fit(X_train_transformed, y_train_true)\n",
    "        print(\"Training: \")\n",
    "        self.model.score(X_train_transformed, y_train_true)\n",
    "        print()\n",
    "        print(\"Validation: \")\n",
    "        self.model.score(X_test_transformed, y_test_true)\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "    def test(self, X_train_untransformed, y_train_true, X_test_untransformed, y_test_true,\n",
    "            X_train_tokenized, X_test_tokenized):\n",
    "        for key, val in self.list_of_techniques.items():\n",
    "            if self.tokenized[key]:\n",
    "                X_train_transformed = val.transform(X_train_tokenized)\n",
    "                X_test_transformed = val.transform(X_test_tokenized)\n",
    "            else:\n",
    "                X_train_transformed = val.transform(X_train_untransformed)\n",
    "                X_test_transformed = val.transform(X_test_untransformed)\n",
    "            self.testModel(X_train_transformed, y_train_true, X_test_transformed, y_test_true, text = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d381b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = EmbeddingTester(base_classifier)\n",
    "tester.addEmbeddingTechniques(\n",
    "    \"Count Vectorizer(No stopwords removal)\", \n",
    "    create_ngram_vectorizer(X_train, ngram_range = (1,2))\n",
    ")\n",
    "\n",
    "tester.addEmbeddingTechniques(\n",
    "    \"TFIDF Vectorizer(No stopwords removal)\", \n",
    "    create_tfidf_ngram_vectorizer(X_train, ngram_range = (1,2))\n",
    ")\n",
    "\n",
    "tester.addEmbeddingTechniques(\n",
    "    \"Count Vectorizer(With stopwords removal)\", \n",
    "    create_ngram_vectorizer(X_train, ngram_range = (1,2), stop_words='english')\n",
    ")\n",
    "\n",
    "tester.addEmbeddingTechniques(\n",
    "    \"TFIDF Vectorizer(With stopwords removal)\", \n",
    "    create_tfidf_ngram_vectorizer(X_train, ngram_range = (1,2), stop_words='english')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f73fc255",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Thanks Rama, like srsly\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c72d1be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 128\n",
    "word_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "X_train_tokenized = [word_tokenizer.tokenize(text) for text in X_train]\n",
    "X_val_tokenized = [word_tokenizer.tokenize(text) for text in X_val]\n",
    "\n",
    "model = Word2Vec(X_train_tokenized, min_count = 1, vector_size= vector_size, workers = 3, window = 3, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e21c7b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(X_tokenized):\n",
    "    temp = np.matrix(\n",
    "        [np.mean([model.wv[i] if i in model.wv else np.array([0.0] * vector_size, dtype=np.float64) for i in tokens], axis = 0) for tokens in X_tokenized],\n",
    "        dtype=np.float64\n",
    "    )\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cda0bba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2Vec Mean Embedding\n",
      "Training: \n",
      "Accuracy score: 0.6668909059431652\n",
      "Recall score: 0.6332701110058779\n",
      "Precision score: 0.6789974520030014\n",
      "F1 score: 0.6553370729010993\n",
      "\n",
      "Validation: \n",
      "Accuracy score: 0.6652948567019182\n",
      "Recall score: 0.6293110275193031\n",
      "Precision score: 0.6778083418628454\n",
      "F1 score: 0.6526600004106523\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tester.addEmbeddingTechniques(\n",
    "    \"word2Vec Mean Embedding\", \n",
    "    EmbeddingTechniques(transform),\n",
    "    True\n",
    ")\n",
    "\n",
    "tester.test(X_train, y_train, X_val, y_val, X_train_tokenized, X_val_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0934fbaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
