{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Sarcasm Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_csv_1 = pd.read_csv(\"train-balanced-sarcasm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_csv_1[\"comment\"] = training_csv_1[\"comment\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_csv_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The total training data has {training_csv_1.author.nunique()} rows.\")\n",
    "training_csv_1.groupby(\"author\").mean()[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The authors is mostly 0.5 probability of each label, might consider dropping it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The total training data has {training_csv_1.subreddit.nunique()} rows.\")\n",
    "training_csv_1.groupby(\"subreddit\").mean()[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subreddit seems to provide more info than expected, should probably keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_csv_1[[\"ups\", \"downs\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notice how ups and downs seem to have a correlation? Lets test this theory out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_csv_1[training_csv_1[\"ups\"].apply(lambda x: -1 if x <= -1 else 0) != training_csv_1[\"downs\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Only 6.1% does not follow the rules, is downs worth keeping? Debatable I guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model using Comment Column only (Unigram Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Better abstraction\n",
    "\n",
    "class sklearnClassifier:\n",
    "    def __init__(self, model, data, label, fitBool = True):\n",
    "        self.model = model\n",
    "        if fitBool: self.fit(data, label)\n",
    "            \n",
    "    def fit(self, data, label):\n",
    "        self.model.fit(data, label)\n",
    "    \n",
    "    def score(self, X, y_true):\n",
    "        y_pred = self.model.predict(X)\n",
    "        print(f\"Accuracy score: {accuracy_score(y_true, y_pred)}\")\n",
    "        print(f\"Recall score: {recall_score(y_true, y_pred)}\")\n",
    "        print(f\"Precision score: {precision_score(y_true, y_pred)}\")\n",
    "        print(f\"F1 score: {f1_score(y_true, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_csv_1[\"comment\"] = training_csv_1[\"comment\"].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    training_csv_1[\"comment\"], \n",
    "    training_csv_1[\"label\"], \n",
    "    test_size = 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram_vectorizer(text_train, ngram_range = (1,1), **kwargs):\n",
    "    vectorizer = CountVectorizer(ngram_range = ngram_range, **kwargs)\n",
    "    vectorizer.fit(text_train)\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_vectorizer = create_ngram_vectorizer(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = unigram_vectorizer.transform(X_train)\n",
    "X_val_transformed = unigram_vectorizer.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_classifier = sklearnClassifier(SGDClassifier(), X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training: \")\n",
    "base_classifier.score(X_train_transformed, y_train)\n",
    "print(\"Validation: \")\n",
    "base_classifier.score(X_val_transformed, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now what? Bigrams and Trigrams, LETZ GO!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1, 3): # Trigram is a bit slow so we'll bring that back later\n",
    "#     igram_vectorizer = create_ngram_vectorizer(X_train, ngram_range = (1,i))\n",
    "#     X_train_transformed = igram_vectorizer.transform(X_train)\n",
    "#     X_val_transformed = igram_vectorizer.transform(X_val)\n",
    "    \n",
    "#     base_classifier = sklearnClassifier(SGDClassifier(), X_train_transformed, y_train)\n",
    "    \n",
    "#     print(\"Training: \")\n",
    "#     base_classifier.score(X_train_transformed, y_train)\n",
    "#     print(\"Validation: \")\n",
    "#     base_classifier.score(X_val_transformed, y_val)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TFIDF instead of just counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_ngram_vectorizer(text_train, ngram_range = (1,1), **kwargs):\n",
    "    vectorizer = TfidfVectorizer(ngram_range = ngram_range, **kwargs)\n",
    "    vectorizer.fit(text_train)\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i in range(1,3):\n",
    "#     tfidf_igram_vectorizer = create_tfidf_ngram_vectorizer(X_train, ngram_range = (1,i))\n",
    "#     X_train_transformed = tfidf_igram_vectorizer.transform(X_train)\n",
    "#     X_val_transformed = tfidf_igram_vectorizer.transform(X_val)\n",
    "    \n",
    "#     base_classifier = sklearnClassifier(SGDClassifier(), X_train_transformed, y_train)\n",
    "    \n",
    "#     print(\"Training: \")\n",
    "#     base_classifier.score(X_train_transformed, y_train)\n",
    "#     print(\"Validation: \")\n",
    "#     base_classifier.score(X_val_transformed, y_val)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Representation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Abstraction for easier work\n",
    "class EmbeddingTechniques:\n",
    "    def __init__(self, method):\n",
    "        self.transformMethod = method\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.transformMethod(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingTester:\n",
    "    def __init__(self, sklearnmodel):\n",
    "        self.list_of_techniques = {}\n",
    "        self.tokenized = {}\n",
    "        self.model = sklearnmodel\n",
    "        \n",
    "    def addEmbeddingTechniques(self, key, method, tokenized = False):\n",
    "        self.list_of_techniques[key] = method\n",
    "        self.tokenized[key] = tokenized\n",
    "        \n",
    "        \n",
    "    def testModel(self, X_train_transformed, y_train_true, X_test_transformed, y_test_true, text = None):\n",
    "        if text is not None: print(text)\n",
    "        self.model.fit(X_train_transformed, y_train_true)\n",
    "        print(\"Training: \")\n",
    "        self.model.score(X_train_transformed, y_train_true)\n",
    "        print()\n",
    "        print(\"Validation: \")\n",
    "        self.model.score(X_test_transformed, y_test_true)\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "    def test(self, X_train_untransformed, y_train_true, X_test_untransformed, y_test_true,\n",
    "            X_train_tokenized, X_test_tokenized):\n",
    "        for key, val in self.list_of_techniques.items():\n",
    "            if self.tokenized[key]:\n",
    "                X_train_transformed = val.transform(X_train_tokenized)\n",
    "                X_test_transformed = val.transform(X_test_tokenized)\n",
    "            else:\n",
    "                X_train_transformed = val.transform(X_train_untransformed)\n",
    "                X_test_transformed = val.transform(X_test_untransformed)\n",
    "            self.testModel(X_train_transformed, y_train_true, X_test_transformed, y_test_true, text = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = EmbeddingTester(base_classifier)\n",
    "tester.addEmbeddingTechniques(\n",
    "    \"Count Vectorizer(No stopwords removal)\", \n",
    "    create_ngram_vectorizer(X_train, ngram_range = (1,2))\n",
    ")\n",
    "\n",
    "tester.addEmbeddingTechniques(\n",
    "    \"TFIDF Vectorizer(No stopwords removal)\", \n",
    "    create_tfidf_ngram_vectorizer(X_train, ngram_range = (1,2))\n",
    ")\n",
    "\n",
    "tester.addEmbeddingTechniques(\n",
    "    \"Count Vectorizer(With stopwords removal)\", \n",
    "    create_ngram_vectorizer(X_train, ngram_range = (1,2), stop_words='english')\n",
    ")\n",
    "\n",
    "tester.addEmbeddingTechniques(\n",
    "    \"TFIDF Vectorizer(With stopwords removal)\", \n",
    "    create_tfidf_ngram_vectorizer(X_train, ngram_range = (1,2), stop_words='english')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Thanks Rama, like srsly\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 128\n",
    "word_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "X_train_tokenized = [word_tokenizer.tokenize(text) for text in X_train]\n",
    "X_val_tokenized = [word_tokenizer.tokenize(text) for text in X_val]\n",
    "\n",
    "model = Word2Vec(X_train_tokenized, min_count = 1, vector_size= vector_size, workers = 3, window = 3, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(X_tokenized):\n",
    "    temp = np.matrix(\n",
    "        [np.mean([model.wv[i] if i in model.wv else np.array([0.0] * vector_size, dtype=np.float64) for i in tokens], axis = 0) for tokens in X_tokenized],\n",
    "        dtype=np.float64\n",
    "    )\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.addEmbeddingTechniques(\n",
    "    \"word2Vec Mean Embedding\", \n",
    "    EmbeddingTechniques(transform),\n",
    "    True\n",
    ")\n",
    "\n",
    "tester.test(X_train, y_train, X_val, y_val, X_train_tokenized, X_val_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1123)>\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer, WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "porter_stemmer = PorterStemmer()\n",
    "word_tokenizer = TreebankWordTokenizer()\n",
    "word_tokenizer2 = WordPunctTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenize cols\n",
    "## Define function to remove stopwords and tokenize comments and or parent_comments\n",
    "def create_stopwords_dict():\n",
    "  stopwords_dict = {}\n",
    "  for word in set(stopwords.words('english')):\n",
    "    stopwords_dict[word] = True\n",
    "  return stopwords_dict\n",
    "\n",
    "stop_words_dict = create_stopwords_dict()\n",
    "\n",
    "\n",
    "def remove_stopwords_and_tokenize(text):\n",
    "  arr = word_tokenizer.tokenize(text)\n",
    "  arr = [word for word in arr if word not in stop_words_dict]\n",
    "  return arr\n",
    "\n",
    "def remove_stopwords_and_tokenize_cols_in_dataset(dataset, cols):\n",
    "    for col in cols:\n",
    "        dataset.dropna(subset=[col], inplace=True)\n",
    "        dataset[col] = training_csv_1[col].apply(lambda x: remove_stopwords_and_tokenize(x))\n",
    "        dataset[col] = training_csv_1[col].apply(lambda x: remove_stopwords_and_tokenize(x))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After tokenization\n",
    "## Define function to add length of comments and or parent comments\n",
    "def add_length_feature_to_dataset(dataset, cols):\n",
    "    for col in cols:\n",
    "        new_col = \"num_\" + col + \"_words\" \n",
    "        dataset[new_col] = dataset[col].apply(lambda x: len(x))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function that splits training set into just sarcasm and just non-sarcasm\n",
    "def split_training_dataset_into_separate_labels(training_dataset):\n",
    "    sarcasm = training_dataset[training_dataset['label'] == 1]\n",
    "    non_sarcasm = training_dataset[training_dataset['label'] == 0]\n",
    "    return sarcasm, non_sarcasm\n",
    "\n",
    "## Define function to engineer features for model such as subreddit history and author history\n",
    "def feature_history(training_dataset, col):\n",
    "    history_sarcasm = {}\n",
    "    history_non_sarcasm = {}\n",
    "    \n",
    "    total_comments_by_feature_history = {}\n",
    "    proportion_sarcasm_by_feature_history = {}\n",
    "    \n",
    "    for index, row in training_dataset.iterrows():\n",
    "        if int(row['label']) == 1:\n",
    "            if row[col] not in history_sarcasm:\n",
    "                history_sarcasm[row[col]] = 0\n",
    "                history_non_sarcasm[row[col]] = 0\n",
    "            history_sarcasm[row[col]] += 1\n",
    "    \n",
    "        elif int(row['label']) == 0:\n",
    "            if row[col] not in history_non_sarcasm:\n",
    "                history_non_sarcasm[row[col]] = 0\n",
    "                history_sarcasm[row[col]] = 0\n",
    "            history_non_sarcasm[row[col]] += 1\n",
    "    \n",
    "    for val in history_sarcasm.keys():\n",
    "        num_sarcasm = history_sarcasm[val]\n",
    "        num_non_sarcasm = history_non_sarcasm[val]\n",
    "        total_comments = num_sarcasm + num_non_sarcasm\n",
    "        sarcasm_proportion = num_sarcasm/total_comments\n",
    "        \n",
    "        proportion_sarcasm_by_feature_history[val] = sarcasm_proportion\n",
    "        total_comments_by_feature_history[val] = total_comments\n",
    "    \n",
    "    return proportion_sarcasm_by_feature_history, total_comments_by_feature_history\n",
    "\n",
    "\n",
    "\n",
    "## Define function to prepare training dataset\n",
    "\n",
    "def add_feature_history_to_train(train_dataset, col):\n",
    "    (proportion_history, total_comments_history) = feature_history(train_dataset, col)\n",
    "    proportion_col = \"sarcasm_proportion_by_\" + col\n",
    "    total_col = \"total_num_comments_by_\" + col\n",
    "    \n",
    "    train_dataset[proportion_col] = train_dataset[col].apply(lambda x: proportion_history[x])\n",
    "    train_dataset[total_col] = train_dataset[col].apply(lambda x: total_comments_history[x])\n",
    "    \n",
    "    return train_dataset\n",
    "\n",
    "## Define function to prepare testing dataset\n",
    "\n",
    "def calculate_mean(table):\n",
    "    values = table.values()\n",
    "    return sum(values)/(len(values))\n",
    "\n",
    "def add_feature_history_to_test(test_dataset, col, proportion_history, total_comments_history):\n",
    "    default_proportion = calculate_mean(proportion_history)\n",
    "    default_total_comments = calculate_mean(total_comments_history)\n",
    "    \n",
    "    def getProportion(col_val):\n",
    "        proportion = default_proportion\n",
    "        if col_val in proportion_history:\n",
    "            proportion = proportion_history[col_val]\n",
    "    \n",
    "        return proportion\n",
    "    \n",
    "    def getTotal(col_val):\n",
    "        total = default_total_comments\n",
    "        if col_val in total_comments_history:\n",
    "            total = total_comments_history[col_val]\n",
    "        \n",
    "        return total\n",
    "    \n",
    "    proportion_col = \"sarcasm_proportion_by_\" + col\n",
    "    total_col = \"total_num_comments_by_\" + col\n",
    "    \n",
    "    test_dataset[proportion_col] = test_dataset[col].apply(lambda x: getProportion(x))\n",
    "    test_dataset[total_col] = test_dataset[col].apply(lambda x: getTotal(x))\n",
    "    \n",
    "    return test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before tokenizing\n",
    "## Counting number of exclamation marks\n",
    "def count_num_exclamation_marks(text):\n",
    "    return text.count(\"!\")\n",
    "        \n",
    "def add_num_exclamation_mark_in_feature(dataset, cols):\n",
    "    for col in cols:\n",
    "        dataset[col + \"_num_exclamation_marks\"] = dataset[col].apply(lambda x: count_num_exclamation_marks(x))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before tokenizing\n",
    "## Counting number of repeated exclamation marks\n",
    "def count_num_repeated_explanation_marks(text):\n",
    "    return text.count(\"!!\")\n",
    "\n",
    "def add_num_repeated_exclamation_mark_in_feature(dataset, cols):\n",
    "    for col in cols:\n",
    "        dataset[col + \"_num_repeated_exclamation_marks\"] = dataset[col].apply(lambda x: count_num_repeated_explanation_marks(x))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before tokenizing\n",
    "## Count number of emoticons\n",
    "def count_num_common_emoticons(text):\n",
    "    common_emoticons = [\":(\", \":)\", \"<3\", \":'(\", \":')\", \"):\", \"(:\"]\n",
    "    count = 0\n",
    "    for emoticon in common_emoticons:\n",
    "        count += text.count(emoticon)\n",
    "    return count\n",
    "\n",
    "def add_num_emoticons_in_feature(dataset, cols):\n",
    "    for col in cols:\n",
    "        dataset[col + \"_num_emoticons\"] = dataset[col].apply(lambda x: count_num_common_emoticons(x))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before tokenizing\n",
    "## Count number of common \"slang\" style abbreviations\n",
    "def count_num_common_slang(text):\n",
    "    common_slang = [\"kms\", \"smh\", \"smdh\", \"smfh\", \"rofl\", \"roflmao\", \"sic \", \"lol\"]\n",
    "    count = 0\n",
    "    for slang in common_slang:\n",
    "        count += text.count(slang)\n",
    "    return count\n",
    "\n",
    "def add_num_slang_in_feature(dataset, cols):\n",
    "    for col in cols:\n",
    "        dataset[col + \"_num_slang\"] = dataset[col].apply(lambda x: count_num_common_slang(x))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After tokenizing\n",
    "## Count number of misspelled words\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "spellchecker = SpellChecker(language=\"en\")\n",
    "\n",
    "def count_number_of_misspelled_words(text):\n",
    "    count = 0\n",
    "    misspelled_words = spellchecker.unknown(text)\n",
    "    return len(misspelled_words)\n",
    "\n",
    "def add_num_misspelled_words_feature(dataset, cols):\n",
    "    for col in cols:\n",
    "        dataset[col + \"_num_misspelled_words\"] = dataset[col].apply(lambda x: count_number_of_misspelled_words)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After tokenizing\n",
    "## Measure misspelling in a different way - by summing up edit distances\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "def measure_sum_of_edit_distances(text):\n",
    "    distances = 0\n",
    "    misspelled_words = spellchecker.unknown(text)\n",
    "    for misspelled_word in misspelled_words:\n",
    "        corrected_word = spellchecker.correction(misspelled_word)\n",
    "        distances += edit_distance(corrected_word, misspelled_word)\n",
    "    return distances\n",
    "\n",
    "def add_sum_of_edit_distances_feature(dataset, cols):\n",
    "    for col in cols:\n",
    "        dataset[col + \"_edit_distance_misspelled_words\"] = dataset[col].apply(lambda x: measure_sum_of_edit_distances(x))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load CSV\n",
    "training_csv_feature_engineering = pd.read_csv(\"train-balanced-sarcasm.csv\")\n",
    "## training_csv_feature_engineering.dropna(subset=['comment', 'parent_comment'], inplace=True)\n",
    "training_csv_feature_engineering[\"comment\"] = training_csv_feature_engineering[\"comment\"].astype(str)\n",
    "training_csv_feature_engineering[\"parent_comment\"] = training_csv_feature_engineering[\"parent_comment\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT_AND_PARENT_COMMENT = [\"comment\", \"parent_comment\"]\n",
    "COMMENT = [\"comment\"]\n",
    "PARENT_COMMENT = [\"parent_comment\"]\n",
    "AUTHOR = \"author\"\n",
    "SUBREDDIT = \"subreddit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add BEFORE tokenization features\n",
    "training_csv_feature_engineering = add_num_exclamation_mark_in_feature(\n",
    "    training_csv_feature_engineering, COMMENT_AND_PARENT_COMMENT)\n",
    "\n",
    "training_csv_feature_engineering = add_num_repeated_exclamation_mark_in_feature(\n",
    "    training_csv_feature_engineering, COMMENT_AND_PARENT_COMMENT)\n",
    "\n",
    "training_csv_feature_engineering = add_num_emoticons_in_feature(\n",
    "    training_csv_feature_engineering, COMMENT_AND_PARENT_COMMENT)\n",
    "\n",
    "training_csv_feature_engineering = add_num_slang_in_feature(\n",
    "    training_csv_feature_engineering, COMMENT_AND_PARENT_COMMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenize\n",
    "training_csv_feature_engineering = remove_stopwords_and_tokenize_cols_in_dataset(\n",
    "    training_csv_feature_engineering, COMMENT_AND_PARENT_COMMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add AFTER tokenization features\n",
    "training_csv_feature_engineering = add_length_feature_to_dataset(\n",
    "    training_csv_feature_engineering, COMMENT_AND_PARENT_COMMENT)\n",
    "\n",
    "training_csv_feature_engineering = add_feature_history_to_train(\n",
    "    training_csv_feature_engineering, AUTHOR)\n",
    "\n",
    "training_csv_feature_engineering = add_feature_history_to_train(\n",
    "    training_csv_feature_engineering, SUBREDDIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalise added features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler() \n",
    "scaled_values = scaler.fit_transform(training_csv_feature_engineering.iloc[:, 10:]) \n",
    "training_csv_feature_engineering.iloc[:,10:] = scaled_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preview of features\n",
    "## Engineered features from column 10 to end (0 based indexing)\n",
    "training_csv_feature_engineering.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}